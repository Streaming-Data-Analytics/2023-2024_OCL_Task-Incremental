{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partire da avalanche 0.4.0 con l'auito del notebook \"prova_OCL\" tentiamo di costruire un benchmark con esperienze correttamente inserite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/SDAenv3/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from avalanche.benchmarks.classic import SplitCIFAR10\n",
    "from avalanche.benchmarks.scenarios import OnlineCLScenario\n",
    "from avalanche.models import SimpleCNN, SimpleMLP\n",
    "from avalanche.training.plugins import EvaluationPlugin\n",
    "from avalanche.evaluation.metrics import accuracy_metrics,\\\n",
    "    forward_transfer_metrics,bwt_metrics,\\\n",
    "    ram_usage_metrics, timing_metrics, EpochAccuracy\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from avalanche.training.plugins import EarlyStoppingPlugin\n",
    "from torch.optim import SGD\n",
    "import torch\n",
    "# Install necessary packages (uncomment if running in a new environment)\n",
    "# !pip install avalanche-lib==0.4.0 torch==2.1.2 matplotlib\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from avalanche.benchmarks.classic import SplitCIFAR10\n",
    "from avalanche.training.plugins import EvaluationPlugin\n",
    "from avalanche.training.supervised import Naive\n",
    "from avalanche.evaluation.metrics import accuracy_metrics, loss_metrics, timing_metrics, forgetting_metrics, cpu_usage_metrics\n",
    "from avalanche.logging import InteractiveLogger\n",
    "from avalanche.training.plugins import EarlyStoppingPlugin\n",
    "from avalanche.benchmarks.scenarios import OnlineCLScenario\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import the SlimResNet18 from the provided file\n",
    "from slim_resnet18 import SlimResNet18\n",
    "\n",
    "# Create model function\n",
    "def create_model():\n",
    "    return SlimResNet18(nclasses=10, input_size=(3, 32, 32))\n",
    "\n",
    "# Optimizer\n",
    "def build_optimizer(model):\n",
    "    return optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Loss function\n",
    "def build_criterion():\n",
    "    return nn.CrossEntropyLoss()\n",
    "\n",
    "# Evaluation plugin\n",
    "def build_eval_plugin():\n",
    "    return EvaluationPlugin(\n",
    "        accuracy_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "        loss_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "        timing_metrics(epoch=True, stream=True),\n",
    "        forgetting_metrics(experience=True, stream=True),\n",
    "        cpu_usage_metrics(experience=True, stream=True),\n",
    "        loggers=[InteractiveLogger()]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark ###\n",
    "\n",
    "ci occupiamo ora dell'inizializzazione del benchmark a partire da CLEAR10.\n",
    "Colleghiamo la cartella drive dove conserviamo il file pickle e numpy delle nostre immagini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Carica da file .npy\n",
    "#CLEAR10_train = np.load('/content/drive/Othercomputers/Il mio MacBook Pro/cartella drive/CLEAR10_train.npy', allow_pickle=True).item()\n",
    "\n",
    "# Carica da file .pickle\n",
    "with open('CLEAR10_train.pickle', 'rb') as handle:\n",
    "   CLEAR10_train = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importiamo le librerie per inizializzare il benchmark. A questo scopo sono necessari tensori torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data.dataset import TensorDataset\n",
    "from avalanche.benchmarks.utils import make_classification_dataset\n",
    "from avalanche.benchmarks.generators import dataset_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEAR10_train = CLEAR10_train\n",
    "tensor_data = torch.tensor(CLEAR10_train, dtype = torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inizializziamo allora due vettori di train e test per conservare i dati (non credo sia strettamente necessario salvarli, da valutare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_experiences = []\n",
    "test_experiences = []\n",
    "torch_datas = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "andiamo allora ad assemblare le componenti per il benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "  x_data = tensor_data[i*3300:(i+1)*3300]\n",
    "  task_dim = x_data.size()[0] # necessario per compatibilità di dimensioni\n",
    "  y_data = torch.full((task_dim,), i) # task labels\n",
    "  torch_data = TensorDataset(x_data, y_data)\n",
    "  torch_datas.append(torch_data)\n",
    "  dataset = make_classification_dataset(torch_data, task_labels=[i for _ in range(task_dim)])\n",
    "  train_experiences.append(dataset)\n",
    "\n",
    "  x_data_test = tensor_data[i*3300:(i+1)*3300]\n",
    "  y_data_test = torch.full((task_dim,), i) # task labels\n",
    "  torch_data_test = TensorDataset(x_data_test, y_data_test)\n",
    "  dataset_test = make_classification_dataset(torch_data_test, task_labels=[i for _ in range(task_dim)])\n",
    "  test_experiences.append(dataset_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "passiamo alla vera e propria creazione del benchmark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark = dataset_benchmark(train_datasets = train_experiences, test_datasets = test_experiences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modello ###\n",
    "\n",
    "importiamo ora il modello Naive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creiamo il base model, l'evaluation plugin (necessario a definire le metriche da considerare), l'optimizer ed infine una funzione per la loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of experience  0\n",
      "Current Classes  [0]\n",
      "Minibatch:  1\n",
      "<avalanche.benchmarks.scenarios.online_scenario.OnlineClassificationExperience object at 0x11f2d69d0>\n",
      "-- >> Start of training phase << --\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 0 (Task 0) from test stream --\n",
      "100%|██████████| 330/330 [00:22<00:00, 14.57it/s]\n",
      "> Eval on experience 0 (Task 0) from test stream ended.\n",
      "\tCPUUsage_Exp/eval_phase/test_stream/Task000/Exp000 = 105.1656\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp000 = 6.6482\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp000 = 0.0000\n",
      "-- Starting eval on experience 1 (Task 1) from test stream --\n",
      "100%|██████████| 330/330 [00:14<00:00, 22.20it/s]\n",
      "> Eval on experience 1 (Task 1) from test stream ended.\n",
      "\tCPUUsage_Exp/eval_phase/test_stream/Task001/Exp001 = 147.7725\n",
      "\tLoss_Exp/eval_phase/test_stream/Task001/Exp001 = 1.8062\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task001/Exp001 = 0.0003\n",
      "-- Starting eval on experience 2 (Task 2) from test stream --\n",
      "100%|██████████| 330/330 [00:17<00:00, 18.91it/s]\n",
      "> Eval on experience 2 (Task 2) from test stream ended.\n",
      "\tCPUUsage_Exp/eval_phase/test_stream/Task002/Exp002 = 127.2053\n",
      "\tLoss_Exp/eval_phase/test_stream/Task002/Exp002 = 7.0566\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task002/Exp002 = 0.0000\n",
      "-- Starting eval on experience 3 (Task 3) from test stream --\n",
      "100%|██████████| 330/330 [00:16<00:00, 19.74it/s]\n",
      "> Eval on experience 3 (Task 3) from test stream ended.\n",
      "\tCPUUsage_Exp/eval_phase/test_stream/Task003/Exp003 = 133.9155\n",
      "\tLoss_Exp/eval_phase/test_stream/Task003/Exp003 = 4.9658\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task003/Exp003 = 0.0000\n",
      "-- Starting eval on experience 4 (Task 4) from test stream --\n",
      "100%|██████████| 330/330 [00:10<00:00, 30.19it/s]\n",
      "> Eval on experience 4 (Task 4) from test stream ended.\n",
      "\tCPUUsage_Exp/eval_phase/test_stream/Task004/Exp004 = 168.0118\n",
      "\tLoss_Exp/eval_phase/test_stream/Task004/Exp004 = 1.7846\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task004/Exp004 = 0.0027\n",
      "-- Starting eval on experience 5 (Task 5) from test stream --\n",
      "100%|██████████| 330/330 [00:13<00:00, 24.20it/s]\n",
      "> Eval on experience 5 (Task 5) from test stream ended.\n",
      "\tCPUUsage_Exp/eval_phase/test_stream/Task005/Exp005 = 148.8314\n",
      "\tLoss_Exp/eval_phase/test_stream/Task005/Exp005 = 5.5680\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task005/Exp005 = 0.0000\n",
      "-- Starting eval on experience 6 (Task 6) from test stream --\n",
      "100%|██████████| 330/330 [00:16<00:00, 19.54it/s]\n",
      "> Eval on experience 6 (Task 6) from test stream ended.\n",
      "\tCPUUsage_Exp/eval_phase/test_stream/Task006/Exp006 = 129.0538\n",
      "\tLoss_Exp/eval_phase/test_stream/Task006/Exp006 = 0.6384\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task006/Exp006 = 0.9948\n",
      "-- Starting eval on experience 7 (Task 7) from test stream --\n",
      "100%|██████████| 330/330 [00:16<00:00, 19.56it/s]\n",
      "> Eval on experience 7 (Task 7) from test stream ended.\n",
      "\tCPUUsage_Exp/eval_phase/test_stream/Task007/Exp007 = 139.6604\n",
      "\tLoss_Exp/eval_phase/test_stream/Task007/Exp007 = 4.7178\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task007/Exp007 = 0.0000\n",
      "-- Starting eval on experience 8 (Task 8) from test stream --\n",
      "100%|██████████| 330/330 [00:26<00:00, 12.23it/s]\n",
      "> Eval on experience 8 (Task 8) from test stream ended.\n",
      "\tCPUUsage_Exp/eval_phase/test_stream/Task008/Exp008 = 95.1728\n",
      "\tLoss_Exp/eval_phase/test_stream/Task008/Exp008 = 3.5843\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task008/Exp008 = 0.0000\n",
      "-- Starting eval on experience 9 (Task 9) from test stream --\n",
      " 63%|██████▎   | 207/329 [00:10<00:07, 15.93it/s]"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = create_model()\n",
    "\n",
    "# Define the strategy\n",
    "cl_strategy = Naive(\n",
    "    model,\n",
    "    build_optimizer(model),\n",
    "    build_criterion(),\n",
    "    train_mb_size=10,\n",
    "    train_epochs=1,\n",
    "    eval_mb_size=10,\n",
    "    evaluator=build_eval_plugin(),\n",
    "    eval_every=1,\n",
    ")\n",
    "\n",
    "res = []\n",
    "minibatches = [] # salvo i minibatches per vedere\n",
    "for experience in benchmark.train_stream:\n",
    "  res.append([])\n",
    "  print(\"Start of experience \", experience.current_experience)\n",
    "  print(\"Current Classes \", experience.classes_in_this_experience)\n",
    "  ocl_scenario = OnlineCLScenario(\n",
    "      original_streams=benchmark.streams.values(),\n",
    "      experiences=experience,\n",
    "      experience_size=10,\n",
    "      access_task_boundaries=True,\n",
    "      shuffle=False\n",
    "  )\n",
    "\n",
    "  minibatches.append([])\n",
    "  for i, minibatch in enumerate(ocl_scenario.train_stream):\n",
    "    print(\"Minibatch: \", i+1)\n",
    "    print(minibatch)\n",
    "    res[-1].append(cl_strategy.train(minibatch, eval_streams=[benchmark.test_stream]))\n",
    "    minibatches[-1].append(minibatch)\n",
    "  print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SDAenv3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
